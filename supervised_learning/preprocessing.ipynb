{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7649fbd",
   "metadata": {},
   "source": [
    "# Notebook 2 \n",
    "\n",
    "\n",
    "### Preprocessing and building new features from original text\n",
    "\n",
    "This notebook discusses the various steps needed to preprocess the dataset and get it ready for the classification task. The binary classes we are interested in are:\n",
    "\n",
    "- **0** text does NOT need to be simplified \n",
    "- **1** text needs to be simplified*\n",
    "\n",
    "Let's review the steps needed to accomplish the document classification task.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. Problem definition and solution approach (covered in the README file)\n",
    "2. Input data\n",
    "3. Exploratory Data Analysis (Covered in Notebook 1 \"Exploratory Data Analysis notebook\")\n",
    "4. Feature Engineering (covered in this notebook \"Notebook 2\")\n",
    "5. Predictive Models (covered in modeling notebook \"Notebook 3\" and \"Notebook 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e831c3d",
   "metadata": {},
   "source": [
    "## Input data\n",
    "\n",
    "The training data contains 416,768 sentences from simple Wikipedia, already labeled with one of the above categories. There are two columns in the dataset (original text and label).\n",
    "There is an additional data set (test data) that contains 119,092 comments that are unlabeled. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7701ef",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "There are several aspects of a text that influence its readability, including the vocabulary level, the syntactic structure, and overall coherence. Based on findings from Cardoso et al., 2013 and using some intuition of what might make a text easy or hard to read, we settled on the following features:\n",
    "\n",
    "\n",
    "#### Surface Features\n",
    "\n",
    "- Average number of characters per word in the document\n",
    "\n",
    "- Average number of words per sentence in the document\n",
    "\n",
    "- Total number of words in the document\n",
    "\n",
    "#### Syntactic Features\n",
    "\n",
    "According to Kate et al.(2010), syntactic structures appear to affect text complexity level. As Barzilay and Lapata(2008)note, more noun phrases make texts more complex and harder to understand.\n",
    "\n",
    "Example of a noun phrase:\n",
    "**The glowing stars shining with bright colors, took our breath away this evening.**\n",
    "\n",
    "For more information on identifying phrases refer to: https://www.albert.io/blog/identifying-phrases/\n",
    "\n",
    "- Number of noun phrases in the document\n",
    "\n",
    "- Number of verb phrases in the document\n",
    "\n",
    "#### Discourse and coherence features\n",
    "\n",
    "Discourse features can refer to text cohesion and coherence. *Text cohesion* refers to the grammatical and lexical links which connect linguistic entities together. *Text coherence* refers to the connection between ideas (Davoodi & Kosseim, 2016)\n",
    "\n",
    "- Number of pronouns in the document (e.g \"I\", \"he\"..etc)\n",
    "\n",
    "- Number of coordinate and subordinate conjuncts in the document(\"however\", \"consequently\"..etc)\n",
    "\n",
    "#### Readability scores\n",
    "\n",
    "- Age of Acquisition (the approximate age in years when a word was learned). The assumption here is that a word learned at a younger age is simpler than a word learned at a more advanced age. \n",
    "\n",
    "- Flesch Reading Ease Score: measures a text’s complexity level and maps it to an educational level (low scores indicate higher difficulty)\n",
    "\n",
    "- Dale-Chall Readability Score (uses a lookup table of the most commonly used 3000 English words and returns the grade level using the New Dale-Chall Formula)\n",
    "\n",
    "- McAlpine EFLAW Readability Score (Returns a score for the readability of an english text for a foreign learner or English, focusing on the number of miniwords and length of sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc41e562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import pickle\n",
    "import ast\n",
    "\n",
    "# natural language processing\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict, Counter \n",
    "from nltk import pos_tag\n",
    "import spacy\n",
    "\n",
    "#visualization\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "#tracking progress\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "#display complete non-truncated text data\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "# print multiple lines at once\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Textstat is a python package, to calculate statistics\n",
    "# from text to determine readability,complexity and grade level of a particular corpus.\n",
    "# Package can be found at https://pypi.python.org/pypi/textstat\n",
    "import textstat\n",
    "from textstat.textstat import textstatistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5917e72b",
   "metadata": {},
   "source": [
    "First, let's define a function that takes the large file 'WikiLarge_Train.csv' and returns a random sample of the data of size 1000. This will help us experiment with different techniques and classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "id": "4351af6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename='data/WikiLarge_Train.csv', partial=False, size=1000):\n",
    "    \"\"\"\n",
    "    function to take a random sample of the data\n",
    "    if partial, then a random sample of size 'size' is returned\n",
    "    size is the number of lines we take if partial==True\n",
    "    \"\"\"\n",
    "\n",
    "    if partial:\n",
    "        with open(filename, 'r') as fp:\n",
    "            for count, line in enumerate(fp):\n",
    "                pass\n",
    "        length= count\n",
    "        #take a random sample\n",
    "        skip = sorted(random.sample(range(1, length + 1), length - size))\n",
    "        df = pd.read_csv(filename, skiprows=skip)\n",
    "        csv_file= df.to_csv('data/sample.csv', index= False) \n",
    "        return csv_file\n",
    "\n",
    "    df = pd.read_csv(filename)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fe2b03",
   "metadata": {},
   "source": [
    "Now, we are ready to normalize and preprocess the input data. The goal is to remove as much noise as possible. This includes removing non-letter characters, stop words, converting words into lower case, tokenizing and lemmatizing. I debated whether or not to keep stop words but decided to ultimately keep them. My logic is that stop words will be important in extracting discourse and cohesion features.\n",
    "\n",
    "**NOTE** There are many different parameter settings for Vectorizer objects in scikit-learn. Small changes in these settings can result in very different text representations and significant changes in final classifier accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "id": "0ca90018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    text= [word for word in text.lower().split() if not word.startswith('-') and word.isalpha()]\n",
    "    if len(text)> 1:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "id": "371326a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb479fb040db46eca7e2f11f00d81035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/416768 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load dataframe and get the preprocessed text\n",
    "df = load_data(filename='data/WikiLarge_Train.csv', partial=False, size=1000)\n",
    "df['preprocessed']= df['original_text'].progress_apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "id": "ee2d49f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There is manuscript evidence that Austen continued to work on these pieces as late as the period 1809 â '' 11 , and that her niece and nephew , Anna and James Edward Austen , made further additions as late as 1814 .</td>\n",
       "      <td>1</td>\n",
       "      <td>[there, is, manuscript, evidence, that, austen, continued, to, work, on, these, pieces, as, late, as, the, period, â, and, that, her, niece, and, nephew, anna, and, james, edward, austen, made, further, additions, as, late, as]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In a remarkable comparative analysis , Mandaean scholar Säve-Söderberg demonstrated that Mani 's Psalms of Thomas were closely related to Mandaean texts .</td>\n",
       "      <td>1</td>\n",
       "      <td>[in, a, remarkable, comparative, analysis, mandaean, scholar, demonstrated, that, mani, psalms, of, thomas, were, closely, related, to, mandaean, texts]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Before Persephone was released to Hermes , who had been sent to retrieve her , Hades tricked her into eating pomegranate seeds , -LRB- six or three according to the telling -RRB- which forced her to return to the underworld for a period each year .</td>\n",
       "      <td>1</td>\n",
       "      <td>[before, persephone, was, released, to, hermes, who, had, been, sent, to, retrieve, her, hades, tricked, her, into, eating, pomegranate, seeds, six, or, three, according, to, the, telling, which, forced, her, to, return, to, the, underworld, for, a, period, each, year]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cogeneration plants are commonly found in district heating systems of cities , hospitals , prisons , oil refineries , paper mills , wastewater treatment plants , thermal enhanced oil recovery wells and industrial plants with large heating needs .</td>\n",
       "      <td>1</td>\n",
       "      <td>[cogeneration, plants, are, commonly, found, in, district, heating, systems, of, cities, hospitals, prisons, oil, refineries, paper, mills, wastewater, treatment, plants, thermal, enhanced, oil, recovery, wells, and, industrial, plants, with, large, heating, needs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Geneva -LRB- , ; , ; , ; ; -RRB- is the second-most-populous city in Switzerland -LRB- after Zürich -RRB- and is the most populous city of Romandie -LRB- the French-speaking part of Switzerland -RRB- .</td>\n",
       "      <td>1</td>\n",
       "      <td>[geneva, is, the, city, in, switzerland, after, zürich, and, is, the, most, populous, city, of, romandie, the, part, of, switzerland]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                              original_text  \\\n",
       "0                                   There is manuscript evidence that Austen continued to work on these pieces as late as the period 1809 â '' 11 , and that her niece and nephew , Anna and James Edward Austen , made further additions as late as 1814 .   \n",
       "1                                                                                                In a remarkable comparative analysis , Mandaean scholar Säve-Söderberg demonstrated that Mani 's Psalms of Thomas were closely related to Mandaean texts .   \n",
       "2  Before Persephone was released to Hermes , who had been sent to retrieve her , Hades tricked her into eating pomegranate seeds , -LRB- six or three according to the telling -RRB- which forced her to return to the underworld for a period each year .   \n",
       "3    Cogeneration plants are commonly found in district heating systems of cities , hospitals , prisons , oil refineries , paper mills , wastewater treatment plants , thermal enhanced oil recovery wells and industrial plants with large heating needs .   \n",
       "4                                                 Geneva -LRB- , ; , ; , ; ; -RRB- is the second-most-populous city in Switzerland -LRB- after Zürich -RRB- and is the most populous city of Romandie -LRB- the French-speaking part of Switzerland -RRB- .   \n",
       "\n",
       "   label  \\\n",
       "0      1   \n",
       "1      1   \n",
       "2      1   \n",
       "3      1   \n",
       "4      1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                    preprocessed  \n",
       "0                                            [there, is, manuscript, evidence, that, austen, continued, to, work, on, these, pieces, as, late, as, the, period, â, and, that, her, niece, and, nephew, anna, and, james, edward, austen, made, further, additions, as, late, as]  \n",
       "1                                                                                                                       [in, a, remarkable, comparative, analysis, mandaean, scholar, demonstrated, that, mani, psalms, of, thomas, were, closely, related, to, mandaean, texts]  \n",
       "2  [before, persephone, was, released, to, hermes, who, had, been, sent, to, retrieve, her, hades, tricked, her, into, eating, pomegranate, seeds, six, or, three, according, to, the, telling, which, forced, her, to, return, to, the, underworld, for, a, period, each, year]  \n",
       "3      [cogeneration, plants, are, commonly, found, in, district, heating, systems, of, cities, hospitals, prisons, oil, refineries, paper, mills, wastewater, treatment, plants, thermal, enhanced, oil, recovery, wells, and, industrial, plants, with, large, heating, needs]  \n",
       "4                                                                                                                                          [geneva, is, the, city, in, switzerland, after, zürich, and, is, the, most, populous, city, of, romandie, the, part, of, switzerland]  "
      ]
     },
     "execution_count": 717,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove None values for empty strings\n",
    "df = df.replace(to_replace='None', value=np.nan).dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "id": "c664d6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all the processed data contains words of at least length 2\n",
    "for row in df['preprocessed']:\n",
    "    if len(row)==1:\n",
    "        print(\"found one\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78782c9",
   "metadata": {},
   "source": [
    "Now that we have a new column containing the preprocessed text, we can go ahead and do some fearure extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae2fe84",
   "metadata": {},
   "source": [
    "### Surface Features \n",
    "\n",
    "- Total number of words in a document\n",
    "\n",
    "- Average word length per document\n",
    "\n",
    "- Average number of syllables in a document\n",
    "\n",
    "- Number of uncommon words\n",
    "\n",
    "- Number of difficult words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "id": "231ecf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(text):\n",
    "    \"\"\"\n",
    "    Returns Number of Words in each document\n",
    "    \"\"\"\n",
    "    words = len(text)\n",
    "\n",
    "    return words\n",
    "\n",
    "def avg_word_length(text):\n",
    "    \"\"\"\n",
    "    Calculate the average token length\n",
    "    \"\"\"\n",
    "    try:\n",
    "        lengths = [len(token) for token in text]\n",
    "  \n",
    "        if len(lengths) > 0:\n",
    "            return np.nanmean(lengths)\n",
    "        else:\n",
    "            if None:\n",
    "                return np.nan\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "\n",
    "def syllables_count(text):\n",
    "    \"\"\"\n",
    "    Returns the average number of syllables per doc\n",
    "    \"\"\"\n",
    "    syls=[]\n",
    "    try:\n",
    "        if len(text)>0:\n",
    "            for word in text:\n",
    "                syl= textstatistics().syllable_count(word) \n",
    "                syls.append(syl)\n",
    "            return np.mean(syls)\n",
    "        else:\n",
    "            if None:\n",
    "                return 0\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "def difficult_words(text):\n",
    "    \"\"\"\n",
    "    Return total number of difficult words in each doc\n",
    "    difficult words are those with syllables >= 2\n",
    "    easy_word_set is provide by Textstat as\n",
    "    a list of common words\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        text = [\"\"]\n",
    "    else:\n",
    "        text= \" \".join(text)\n",
    "        diff_words= textstat.difficult_words(text)\n",
    " \n",
    "        return diff_words\n",
    "\n",
    "def uncommon (text):\n",
    "    \"\"\"\n",
    "    Returns words that are not_in_dale_chall\n",
    "    \"\"\"\n",
    "    Dale_Chall_List = pd.read_csv(\"data/dale_chall.txt\")\n",
    "    if text is None:\n",
    "        text = []\n",
    "\n",
    "    n = [w for w in text if w not in list (Dale_Chall_List['a'])]\n",
    "    n1 = len(n)\n",
    "    return n1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "id": "0cf3c0a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d0ccc981144e1ea6cdbd531a803413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/403608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get word count feature\n",
    "df['word_count']=df['preprocessed'].progress_apply(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "id": "491f254d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1673e4b49c3142a59df098babe6afae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/403608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get average word length feature\n",
    "df['avg_word_count']=df['preprocessed'].progress_apply(avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "id": "9750565e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab7f8cc29f924544a5f0e84e875af905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/403608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get average number of characters per word\n",
    "df['syllable_count']=df['preprocessed'].progress_apply(syllables_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "id": "c465ae8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22577980be6c418b907e1a5a6b29c1e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/403608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get the number of uncommon words\n",
    "df['uncommon']= df['preprocessed'].progress_apply(uncommon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "id": "d73a4162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d1a93379b644df95e5051600fe72fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/403608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get the number of difficult words\n",
    "df['difficult_words']= df['preprocessed'].progress_apply(difficult_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0fd9fc",
   "metadata": {},
   "source": [
    "### Save data into intermediate file\n",
    "\n",
    "Once a file is saved into a csv file, lists are converted into strings. I found out that to prevent this from\n",
    "happening, we'd need to serialize the dataframe in pickle format using pd.to_pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "id": "81c028cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"data/five_features.pkl\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13abafdb",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Some applications benefit from stemming such as opic modeling and training a word vector since accurate counts within the window of a word would be disrupted by an irrelevant inflection like a simple plural or present tense inflection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "id": "5417b347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(lst):\n",
    "    \"\"\"\n",
    "    Converts words into stems\n",
    "    \"\"\"\n",
    "    from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "    porter = PorterStemmer()\n",
    " \n",
    "    stem_sentence=[]\n",
    "    for word in res:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "id": "58fee261",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"data/five_features.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "id": "e21cc800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af0154e7503b48929a31f70177f1bb9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/403608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['stem']=df['preprocessed'].progress_apply(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "id": "db03ffc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed</th>\n",
       "      <th>word_count</th>\n",
       "      <th>avg_word_count</th>\n",
       "      <th>syllable_count</th>\n",
       "      <th>uncommon</th>\n",
       "      <th>difficult_words</th>\n",
       "      <th>stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There is manuscript evidence that Austen continued to work on these pieces as late as the period 1809 â '' 11 , and that her niece and nephew , Anna and James Edward Austen , made further additions as late as 1814 .</td>\n",
       "      <td>1</td>\n",
       "      <td>[there, is, manuscript, evidence, that, austen, continued, to, work, on, these, pieces, as, late, as, the, period, â, and, that, her, niece, and, nephew, anna, and, james, edward, austen, made, further, additions, as, late, as]</td>\n",
       "      <td>35</td>\n",
       "      <td>4.485714</td>\n",
       "      <td>1.371429</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>there is manuscript evid that austen continu to work on these piec as late as the period â and that her niec and nephew anna and jame edward austen made further addit as late as</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In a remarkable comparative analysis , Mandaean scholar Säve-Söderberg demonstrated that Mani 's Psalms of Thomas were closely related to Mandaean texts .</td>\n",
       "      <td>1</td>\n",
       "      <td>[in, a, remarkable, comparative, analysis, mandaean, scholar, demonstrated, that, mani, psalms, of, thomas, were, closely, related, to, mandaean, texts]</td>\n",
       "      <td>19</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.789474</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>there is manuscript evid that austen continu to work on these piec as late as the period â and that her niec and nephew anna and jame edward austen made further addit as late as</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                             original_text  \\\n",
       "0  There is manuscript evidence that Austen continued to work on these pieces as late as the period 1809 â '' 11 , and that her niece and nephew , Anna and James Edward Austen , made further additions as late as 1814 .   \n",
       "1                                                               In a remarkable comparative analysis , Mandaean scholar Säve-Söderberg demonstrated that Mani 's Psalms of Thomas were closely related to Mandaean texts .   \n",
       "\n",
       "   label  \\\n",
       "0      1   \n",
       "1      1   \n",
       "\n",
       "                                                                                                                                                                                                                          preprocessed  \\\n",
       "0  [there, is, manuscript, evidence, that, austen, continued, to, work, on, these, pieces, as, late, as, the, period, â, and, that, her, niece, and, nephew, anna, and, james, edward, austen, made, further, additions, as, late, as]   \n",
       "1                                                                             [in, a, remarkable, comparative, analysis, mandaean, scholar, demonstrated, that, mani, psalms, of, thomas, were, closely, related, to, mandaean, texts]   \n",
       "\n",
       "   word_count  avg_word_count  syllable_count  uncommon  difficult_words  \\\n",
       "0          35        4.485714        1.371429        14                7   \n",
       "1          19        6.000000        1.789474        14                8   \n",
       "\n",
       "                                                                                                                                                                                 stem  \n",
       "0  there is manuscript evid that austen continu to work on these piec as late as the period â and that her niec and nephew anna and jame edward austen made further addit as late as   \n",
       "1  there is manuscript evid that austen continu to work on these piec as late as the period â and that her niec and nephew anna and jame edward austen made further addit as late as   "
      ]
     },
     "execution_count": 845,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's check what the output looks like and if it matches our expectations\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a4a987",
   "metadata": {},
   "source": [
    "### Discourse and coherence features\n",
    "\n",
    "- Number of pronouns in the document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "id": "5de455af",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_pronouns = [\"I\", \"you\", \"he\", \"she\", \"it\", \"they\", \"me\", \"you\", \"him\", \"her\", \"it\", \"my\", \"mine\", \\\n",
    "                    \"your\", \"yours\", \"his\", \"her\", \"hers\", \"its\", \"who\", \"whom\", \"whose\", \"what\", \"which\", \\\n",
    "                    \"another\", \"each\", \"everything\", \"nobody\", \"either\", \"someone\", \"who\", \"whom\", \"whose\", \\\n",
    "                    \"that\", \"which\", \"myself\", \"yourself\", \"himself\", \"herself\", \"itself\", \"this\", \"that\"]\n",
    "   \n",
    "english_conjunctions = [\"and\", \"nor\", \"but\", \"or\", \"yet\", \"so\", \"though\", \"although\", \"even though\", \"while\", \\\n",
    "                        \"if\", \"only if\", \"unless\", \"until\", \"provided that\", \"assuming that\", \"even if\", \\\n",
    "                        \"in case\", \"lest\", \"than\", \"rather than\", \"whether\", \"as much as\", \"whereas\", \"after\", \\\n",
    "                        \"as long as\", \"as soon as\", \"before\", \"by the time\", \"now that\", \"once\", \"since\", \"till\",\\\n",
    "                        \"until\", \"when\", \"whenever\", \"while\", \"because\", \"since\", \"so that\", \"in order\", \"why\", \\\n",
    "                        \"that\", \"what\", \"whatever\", \"which\", \"whichever\", \"as though\", \"as if\", \"wherever\", \"also\",\\\n",
    "                        \"besides\", \"furthermore\", \"likewise\", \"moreover\", \"however\", \"nevertheless\", \"nonetheless\",\\\n",
    "                        \"still\", \"conversely\", \"instead\", \"otherwise\", \"rather\", \"accordingly\", \"consequently\",\\\n",
    "                        \"hence\", \"meanwhile\", \"then\", \"therefore\", \"thus\"]\n",
    " \n",
    "def get_discourse(text):\n",
    "    number_of_pronouns=0\n",
    "    for pronoun in english_pronouns:\n",
    "        if pronoun in text:\n",
    "            number_of_pronouns+=1\n",
    "    return number_of_pronouns\n",
    "\n",
    "def cohesive_features(text):\n",
    "    number_of_conjunctions = 0\n",
    "    for conjunct in english_conjunctions:\n",
    "        if conjunct in text:\n",
    "            number_of_conjunctions+=1\n",
    "    return number_of_conjunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "id": "1db72ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e6666e16c2d4b1bb31790f556c25c2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/403608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get the number of pronouns\n",
    "df['discourse']=df['preprocessed'].progress_apply(get_discourse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "id": "cce8e4f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "344fa12e51ab46a489d0c6555cfd4372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/403608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get the number of cohesive features\n",
    "df['cohesive_features']=df['preprocessed'].progress_apply(cohesive_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df13912",
   "metadata": {},
   "source": [
    "#### Readability scores\n",
    "\n",
    "- Age of Acquisition\n",
    "\n",
    "- Flesch Reading Ease Score \n",
    "\n",
    "- Dale-Chall Readability Score  \n",
    "\n",
    "- McAlpine EFLAW Readability Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33c4c151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_aoa(doc):\n",
    "    \"\"\"\n",
    "    Return the mean of the aoa score of a document\n",
    "    \"\"\"\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    # read the file\n",
    "    aoa_df = pd.read_csv('data/AoA_51715_words.csv', encoding='iso-8859-1')\n",
    "    # create a dictionary to hold a word and its score\n",
    "    aoa_dict= aoa_df.set_index('Word').to_dict()['AoA_Kup_lem']\n",
    "    scores = []\n",
    "    for token in doc:\n",
    "        if token.lower() in aoa_dict:\n",
    "            scores.append(aoa_dict[token.lower()])\n",
    "    if scores:\n",
    "        return np.nanmean(scores)\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "    \n",
    "def flesch_reading_ease(text):\n",
    "    \"\"\"\n",
    "        Implements Flesch Formula:\n",
    "        Reading Ease score = 206.835 - (1.015 × ASL) - (84.6 × ASW)\n",
    "        Here,\n",
    "          ASL = average sentence length (number of words\n",
    "                divided by number of sentences)\n",
    "          ASW = average word length in syllables (number of syllables\n",
    "                divided by number of words)\n",
    "    \"\"\"\n",
    "    text= \" \".join(text)\n",
    "    FRE = textstat.flesch_reading_ease(text)\n",
    "    return FRE\n",
    "\n",
    "def dale_chall_readability_score(text):\n",
    "    \"\"\"\n",
    "        Implements Dale Challe Formula:\n",
    "        Raw score = 0.1579*(PDW) + 0.0496*(ASL) + 3.6365\n",
    "        \n",
    "        PDW = Percentage of difficult words.\n",
    "        ASL = Average sentence length\n",
    "    \"\"\"\n",
    "    text= \" \".join(text)\n",
    "    score= textstat.dale_chall_readability_score(text)\n",
    "    return score\n",
    "\n",
    "def mcalpine(text):\n",
    "    text= \" \".join(text)\n",
    "    return textstat.mcalpine_eflaw(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "id": "14094b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc6414ee2584772b436d5599355b60f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/403608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get the Flesch score\n",
    "df['flesch']=df['preprocessed'].progress_apply(flesch_reading_ease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "id": "82f8d025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f46d34552a42d2a0d0a0494d3a21f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/403608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get the Dale Chall score\n",
    "df['dale']=df['preprocessed'].progress_apply(dale_chall_readability_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "id": "c31f55d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7122c167fa9f48d1a64fa01ba2f97267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/403608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get the mcalpine score\n",
    "df['mcalpine']=df['preprocessed'].progress_apply(mcalpine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6f916d",
   "metadata": {},
   "source": [
    "#### Syntactic Features\n",
    "\n",
    "- Proportion of nouns and adjectives in each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "id": "146d27ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#part of speech tags: source https://cs.nyu.edu/~grishman/jet/guide/PennPOS.html\n",
    "\n",
    "def pos_nouns(doc):\n",
    "    \"\"\"\n",
    "    Returns proprotion of nouns and adjectives in NLTK part of speech for each doc\n",
    "    \"\"\"\n",
    "    count=0\n",
    "    word_count=0\n",
    "    for pos in pos_tag(doc):\n",
    "        word_count+=1\n",
    "        if pos[1]=='NN' or pos[1]== 'NNS' or pos[1]=='ADJ':\n",
    "            count+=1\n",
    "    return count/word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "id": "15aa0927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3fa6a9cc5a04a00ba10da9313169506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/403608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['nouns_adjs']=df['preprocessed'].progress_apply(pos_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "id": "7336fd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column that contains the cleaned up text (string as opposed to a list)\n",
    "df['normalized']=df['preprocessed'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "id": "7ef28823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's save the data in a pickle file\n",
    "df.to_pickle(\"data/features.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "id": "36936abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['original_text', 'label', 'preprocessed', 'word_count',\n",
       "       'avg_word_count', 'syllable_count', 'uncommon', 'difficult_words',\n",
       "       'stem', 'discourse', 'cohesive_features', 'flesch', 'dale', 'mcalpine',\n",
       "       'nouns_adjs', 'normalized'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 868,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# columns in the features file\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "id": "87755ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['aoa']=df['preprocessed'].progress_apply(mean_aoa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9dc20b",
   "metadata": {},
   "source": [
    "Function to vectorize the text into tf-idf vectors. \n",
    "\n",
    "We'll define our vectorizer that will learn that mapping and then call the fit transform function to learn how to map our text to a particular matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "id": "7224b89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_vectorize(df):\n",
    "    \"\"\"\n",
    "    Returns vectorized features for training and testing data sets\n",
    "    \"\"\"\n",
    "    #Note that split does not shuffle, so we'll use DataFrame.sample() and randomly resample our entire dataset \n",
    "    #to get a random shuffle before the split.\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    \n",
    "    train_df,dev_df,test_df=np.split(df.sample(frac=1,random_state=42), \n",
    "                                 [int(.8*len(df)), \n",
    "                                  int(.9*len(df))])\n",
    "    \n",
    "    # initialize the vectorizer object and set max features to 1000\n",
    "    vectorizer= TfidfVectorizer(max_features=1000, ngram_range=(1,2), min_df=1) #remove extrememly rare words\n",
    "    # fit transform/only fit test and dev set\n",
    "    X_train=vectorizer.fit_transform(train_df[\"normalized\"])\n",
    "    X_test = vectorizer.fit(test_df[\"normalized\"])\n",
    "    \n",
    "    return X_train,X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "id": "4578dd1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['original_text', 'label', 'preprocessed', 'word_count',\n",
       "       'avg_word_count', 'syllable_count', 'uncommon', 'difficult_words',\n",
       "       'stem', 'discourse', 'cohesive_features', 'flesch', 'dale', 'mcalpine',\n",
       "       'nouns_adjs', 'normalized'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 875,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713fd0dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1008x792 with 0 Axes>"
      ]
     },
     "execution_count": 882,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure(figsize=(14,11))\n",
    "\n",
    "df_plotting= df[['word_count','avg_word_count', 'syllable_count', 'uncommon', 'difficult_words',\\\n",
    "                 'discourse', 'cohesive_features', 'flesch', 'dale', 'mcalpine','nouns_adjs']]\n",
    "ax = sns.pairplot(data = df, plot_kws = dict(color = \"maroon\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "534246ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df2=pd.read_pickle(\"data/features.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ad2e879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4432091163f4cca8b6aafed052174f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/403608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df2['aoa']=df2['preprocessed'].progress_apply(mean_aoa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d82042",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
